{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Deploying a project for offline inference\n",
    "\n",
    "In this notebook, we will show how to create a deployment for a project that can be used to run inference locally, using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "metadata": {},
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75b0fcbf-fe01-44e9-a810-226552d55385",
   "metadata": {},
   "source": [
    "### Selecting a project for deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {},
   "source": [
    "from geti_sdk.rest_clients import ProjectClient\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)\n",
    "projects = project_client.list_projects()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project\n",
    "Let's go with the project we created in notebook [004](004_create_pipeline_project_from_dataset.ipynb): `COCO multitask animal demo`. To create a deployment, we can use the `geti.deploy_project` convenience method. This will download the active (OpenVINO) models for all tasks in the project to our local machine, so that we can use them to run inference locally.\n",
    "\n",
    "> **NOTE**: Downloading the model data may take some time, especially models for anomaly tasks are on the order of 100 Mb in size so please be prepared to wait a bit"
   ]
  },
  {
   "cell_type": "code",
   "id": "8b748042-6d21-471a-8acd-d3f360d543e3",
   "metadata": {},
   "source": [
    "PROJECT_NAME = \"COCO multitask animal demo\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a4c40da3-afeb-42ab-a8bc-be3c66d1053e",
   "metadata": {},
   "source": [
    "Before deploying, we need to make sure that the project is trained. Otherwise it will not contain any models to deploy, and the deployment will fail."
   ]
  },
  {
   "cell_type": "code",
   "id": "ffd5fb89-f4a2-4398-9951-8b1cefa4ab99",
   "metadata": {},
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec4c279c-c6f1-4663-8c41-f96431212252",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below.\n",
    "\n",
    "Note the `enable_explainable_ai` argument. If set to `True`, the deployment will include the necessary artifacts to run the Explainable AI (XAI) service. This will allow us to generate explanations for the predictions made by the models in the deployment."
   ]
  },
  {
   "cell_type": "code",
   "id": "0d17e98e-ac6c-4353-9282-23c2e20835d3",
   "metadata": {},
   "source": [
    "deployment = geti.deploy_project(project_name=PROJECT_NAME, enable_explainable_ai=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3cd8b571-6da5-4618-8f3f-b8576d2d18a2",
   "metadata": {},
   "source": [
    "### Preparing the models for inference\n",
    "Now that the `deployment` is created and the models are saved to the local disk, we can load the models into memory to prepare them for inference. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ab0d01de-6b6a-4174-a033-562034c1374b",
   "metadata": {},
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "430ceb34-04a7-4645-8f39-9edcc0d21446",
   "metadata": {},
   "source": [
    "## Running inference on an image locally\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it.\n",
    "The SDK contains an example image that we use for this. The path to the image is in the `EXAMPLE_IMAGE_PATH` constant, from the `geti_sdk.demos` module."
   ]
  },
  {
   "cell_type": "code",
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {},
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from geti_sdk.demos import EXAMPLE_IMAGE_PATH\n",
    "\n",
    "numpy_image = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
    "\n",
    "# Convert to RGB channel order. All deployed models expect the image in RGB format\n",
    "numpy_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `Visualizer` utility class\n",
    "2. Inspecting its properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {},
   "source": [
    "from geti_sdk import Visualizer\n",
    "\n",
    "visualizer = Visualizer()\n",
    "\n",
    "result = visualizer.draw(numpy_rgb, prediction)\n",
    "visualizer.show_in_notebook(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e2ac65ea",
   "metadata": {},
   "source": [
    "And by printing the prediction overview we can look inside the prediction object structure and properties."
   ]
  },
  {
   "cell_type": "code",
   "id": "f086979f",
   "metadata": {},
   "source": [
    "print(prediction.overview)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c972e8e4",
   "metadata": {},
   "source": [
    "### Explaining the prediction\n",
    "\n",
    "If the deployment was created with the `enable_explainable_ai` argument set to `True`, we can also generate an explanation for the prediction. This can be done using the `deployment.explain` method, which does the inference as the `deployment.infer` method, but also generates saliency maps and adds them to `Prediction` object.\n",
    "\n",
    "Let's generate an explanation for the prediction and visualise it using the `Visualizer` utility class' `explain_label` method."
   ]
  },
  {
   "cell_type": "code",
   "id": "f954741c",
   "metadata": {},
   "source": [
    "t_start = time.time()\n",
    "prediction_with_saliency_map = deployment.explain(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(\n",
    "    f\"Running local inference with XAI on image took {t_elapsed*1000:.2f} milliseconds\"\n",
    ")\n",
    "\n",
    "result = visualizer.explain_label(\n",
    "    numpy_rgb, prediction_with_saliency_map, label_name=\"animal\"\n",
    ")\n",
    "visualizer.show_in_notebook(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Saving the deployment\n",
    "When we create the deployment, the model data is saved to a temporary folder. We store the deployment for offline re-use later on by saving it: This will copy the model data from the temporary folder to the path we specify. If we want to run inference locally again, we can simply reload the deployment from the saved folder, without having to connect to the platform again."
   ]
  },
  {
   "cell_type": "code",
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "PATH_TO_DEPLOYMENT_FOLDER = os.path.join(\"deployments\", PROJECT_NAME)\n",
    "\n",
    "deployment.save(path_to_folder=PATH_TO_DEPLOYMENT_FOLDER)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d28c8272-ef89-4d39-a70f-65f1ccf9cc9e",
   "metadata": {},
   "source": [
    "## Loading a saved deployment\n",
    "Loading a deployment that was previously saved to disk is easy and can be done without establishing a connection to the platform (or without even connecting to the internet, for that matter)."
   ]
  },
  {
   "cell_type": "code",
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {},
   "source": [
    "from geti_sdk.deployment import Deployment\n",
    "\n",
    "offline_deployment = Deployment.from_folder(PATH_TO_DEPLOYMENT_FOLDER)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Again, to prepare the deployment for inference make sure to send the models to CPU (or whichever device you want to use)"
   ]
  },
  {
   "cell_type": "code",
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {},
   "source": [
    "offline_deployment.load_inference_models(device=\"CPU\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d6d4dac8-91cf-4920-8e3e-e8c575d91b46",
   "metadata": {},
   "source": [
    "That's all there is to it! The `offline_deployment` can now be used to run inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5570fd5-3b6d-4fee-9104-8747ab38e6ca",
   "metadata": {},
   "source": [
    "# Comparing local inference and inference on the platform\n",
    "As a final step, we can make a comparison between the local inference results and the predictions sent back from the platform. We will have a look at the time required for both methods, and compare the output."
   ]
  },
  {
   "cell_type": "code",
   "id": "b8293cec-743f-459a-8634-f0d87b0b7601",
   "metadata": {},
   "source": [
    "from geti_sdk.rest_clients import ImageClient, PredictionClient\n",
    "\n",
    "project = project_client.get_project_by_name(PROJECT_NAME)\n",
    "\n",
    "image_client = ImageClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")\n",
    "prediction_client = PredictionClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c2b5-ecb9-4a47-bafc-c6fe136bfb1d",
   "metadata": {},
   "source": [
    "To prepare for platform inference, we have to upload the image to the platform first"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0ca9c4d-a904-4d9a-bf42-5c84d83cf0a3",
   "metadata": {},
   "source": [
    "geti_image = image_client.upload_image(numpy_image)\n",
    "# Load the pixel data to visualize the image later on\n",
    "geti_image.get_data(geti.session);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8ef59a6-baf8-411f-a01b-e0620eee7348",
   "metadata": {},
   "source": [
    "### Comparing inference times\n",
    "Now, we can run inference locally and on the platform, and time both. We will set the prediction client to `ONLINE` mode, which means it will always generate a new prediction for the image, rather than returning cached predictions. Additionally you can set the mode to `AUTO` (which will return cached predictions if available) and re-run the cell to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "id": "0322cd4c-4fb1-42c5-9fa9-547d286b7ca9",
   "metadata": {},
   "source": [
    "from geti_sdk.data_models.enums import PredictionMode\n",
    "\n",
    "prediction_client.mode = PredictionMode.ONLINE\n",
    "\n",
    "# Get platform prediction, and measure time required\n",
    "t_start_platform = time.time()\n",
    "platform_prediction = prediction_client.get_image_prediction(geti_image)\n",
    "t_elapsed_platform = time.time() - t_start_platform\n",
    "\n",
    "# Get local prediction, and measure time required\n",
    "t_start_local = time.time()\n",
    "local_prediction = offline_deployment.infer(numpy_rgb)\n",
    "t_elapsed_local = time.time() - t_start_local\n",
    "\n",
    "print(f\"Platform prediction completed in {t_elapsed_platform*1000:.1f} milliseconds\")\n",
    "print(f\"Local prediction completed in {t_elapsed_local*1000:.1f} milliseconds\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5a17fc3-bcb9-48d1-811f-c02969bf60a2",
   "metadata": {},
   "source": [
    "### Comparing inference results\n",
    "The cell below will show the results from the platform prediction (top) and local prediction (bottom). The two predictions should be equal."
   ]
  },
  {
   "cell_type": "code",
   "id": "65f55ff1-f2ca-4b2d-9dbe-d4e09d6c2b35",
   "metadata": {},
   "source": [
    "geti_image_rgb = cv2.cvtColor(geti_image.numpy, cv2.COLOR_BGR2RGB)\n",
    "platform_result = visualizer.draw(geti_image_rgb, platform_prediction)\n",
    "visualizer.show_in_notebook(platform_result)\n",
    "\n",
    "local_result = visualizer.draw(numpy_rgb, local_prediction)\n",
    "visualizer.show_in_notebook(local_result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05880bf6-43d3-4ecc-afea-39ef5e0597ae",
   "metadata": {},
   "source": [
    "### Cleaning up\n",
    "To clean up, we will delete the geti_image from the project again"
   ]
  },
  {
   "cell_type": "code",
   "id": "26681cc5-fc42-4412-8ea5-af5d57201c37",
   "metadata": {},
   "source": [
    "image_client.delete_images([geti_image])"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

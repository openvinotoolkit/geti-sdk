{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f80ca35-546b-4313-9a19-eac117e417dd",
   "metadata": {},
   "source": [
    "# Serving Geti model(s) with OpenVINO Model Server\n",
    "\n",
    "In this notebook, we will show how to create a stand-alone inference server for a Geti project, using the [OpenVINO Model Server (OVMS)](https://docs.openvino.ai/2021.4/ovms_what_is_openvino_model_server.html). Once the server is running, we'll be able to connect to it through the Geti SDK and send inference requests to it.\n",
    "\n",
    "> NOTE: In this notebook we will run OVMS in a docker container. To make sure you'll be able to follow the notebook smoothly, please ensure that your system has docker installed. You can get Docker Desktop from [here](https://docs.docker.com/get-docker/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9baafb1-4be1-427e-8665-2e9a4d377142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0fcbf-fe01-44e9-a810-226552d55385",
   "metadata": {},
   "source": [
    "### Selecting a project for OVMS deployment\n",
    "Let's list all projects in the workspace and select one for which to create a deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a8fb7-24b8-45ee-aff8-e3044d236756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ProjectClient\n",
    "\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)\n",
    "projects = project_client.list_projects()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e245c2-c7b3-41ed-a99b-15d9a75fd542",
   "metadata": {},
   "source": [
    "## Deploying the project and preparing the OVMS configuration\n",
    "Let's go with the project we created in notebook [002](002_create_project_from_dataset.ipynb): `COCO animal detection demo`. Like in notebook [008](008_deploy_project.ipynb), we can use the `geti.deploy_project` convenience method. This method accepts a `prepare_ovms_config` input parameter, that we can set to `True` to create the required configuration for the OpenVINO Model Server that we intend to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b748042-6d21-471a-8acd-d3f360d543e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"COCO animal detection demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c40da3-afeb-42ab-a8bc-be3c66d1053e",
   "metadata": {},
   "source": [
    "Before deploying, we need to make sure that the project is trained. Otherwise it will not contain any models to deploy, and the deployment will fail.\n",
    "\n",
    "> NOTE: If the `COCO animal detection demo` project does not exist on your Geti server, you can either create it by running notebook [002](002_create_project_from_dataset.ipynb), or select a different project to deploy by changing the `PROJECT_NAME` variable above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5fb89-f4a2-4398-9951-8b1cefa4ab99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4c279c-c6f1-4663-8c41-f96431212252",
   "metadata": {},
   "source": [
    "Once we are sure that the project has trained models for each task, we can create the deployment in the cell below. Note the `prepare_ovms_config=True` argument which indicates that the model configuration for OVMS will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d17e98e-ac6c-4353-9282-23c2e20835d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathvalidate import sanitize_filepath\n",
    "\n",
    "# We'll create a directory with the name of the project to save the deployment to, so we have\n",
    "# to make sure that the project name can act as folder name.\n",
    "safe_project_name = sanitize_filepath(PROJECT_NAME).replace(\" \", \"_\")\n",
    "\n",
    "# Target folder in which to save the deployment and OVMS configuration\n",
    "output_folder = os.path.join(\"deployments\", safe_project_name)\n",
    "\n",
    "# Create the deployment and OVMS configuration, and save it to the `output_folder` on disk\n",
    "deployment = geti.deploy_project(\n",
    "    project_name=PROJECT_NAME, prepare_ovms_config=True, output_folder=output_folder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc8219d-5d29-4120-b2fd-6aef30afaf98",
   "metadata": {},
   "source": [
    "## Setting up the OpenVINO Model Server\n",
    "The cell above should create the deployment for you in the folder `deployments/<PROJECT_NAME>`. You should also see a line stating that the OVMS configuration files for the project have been created. Along with the configuration, a readme file `OVMS_README.md` is included that contains detailed instructions on how to get started. \n",
    "\n",
    "This notebook follows those instructions, going through them step by step without ever having to leave the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3671dd08-c8b7-4cf5-90b2-d09017f9163f",
   "metadata": {},
   "source": [
    "### Launching the OpenVINO Model Server container\n",
    "\n",
    "#### Getting the latest OVMS image\n",
    "The cell below downloads the OVMS docker image to your machine. It assumes you have docker already installed on your system. \n",
    "\n",
    "Note the exclamation mark `!` in front of the statement: This indicates to jupyter that the line to follow is not python syntax, but is a shell command instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84599e9f-4597-4449-b7ec-d33d6132487a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker pull openvino/model_server:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b99923-a29d-4827-bd42-e1018640db7c",
   "metadata": {},
   "source": [
    "#### Running the container for the project\n",
    "The configuration and models for the OVMS container to consume are stored in the `output_folder` that we just specified for the deployment. We need to pass the absolute path to these configuration files to the container when we're running it, so we first have to get the full `ovms_config_path` holding the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7395a5c9-29ae-4778-a884-0b2fbb7d90ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ovms_config_path = os.path.join(os.getcwd(), output_folder, \"ovms_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5be6219-bc0b-461a-88af-f1ccc7cc0df3",
   "metadata": {},
   "source": [
    "The cell below will run launch the docker container with OVMS. It takes its configuration from the deployment we just created, and listens for inference requests on port 9000. If all went well you should see no warnings or errors, only the ID of the container that is created should be printed (something like `aa1b4acfd7a97e2253aa82401056c2ed97934de65a2d51d4324e36dfa84670f1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5171e2ed-4144-4970-9e56-f8447db16a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run -d --rm -v {ovms_config_path}:/models -p 9000:9000 openvino/model_server:latest --port 9000 --config_path /models/ovms_model_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9356047-b4a7-47b9-9e11-fe2507b961bc",
   "metadata": {},
   "source": [
    "## Making inference requests to OVMS\n",
    "### Connecting to OVMS\n",
    "Now that everything is set up and ready, we can connect the deployment we created earlier to the OVMS container that we got running. This is done in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd18ef9-4eb8-4094-9f47-e58bf051e7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfade390-b588-4d11-acaf-b5f8fd2bea33",
   "metadata": {},
   "source": [
    "## Running inference on an image\n",
    "Now, we can load an image as a numpy array (for instance using OpenCV) and use the `deployment.infer` method to generate a prediction for it.\n",
    "The SDK contains an example image that we use for this. The path to the image is in the `EXAMPLE_IMAGE_PATH` constant, from the `geti_sdk.demos` module.\n",
    "\n",
    "If you have worked through notebook [008](008_deploy_project.ipynb) you'll notice that the API for local inference or OVMS inference is exactly the same. The only difference being the target for loading the inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dbf77-b18b-4d86-9787-513ee9d1f328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from geti_sdk.demos import EXAMPLE_IMAGE_PATH\n",
    "\n",
    "numpy_image = cv2.imread(EXAMPLE_IMAGE_PATH)\n",
    "\n",
    "# Convert to RGB channel order. All deployed models expect the image in RGB format\n",
    "numpy_rgb = cv2.cvtColor(numpy_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running OVMS inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a69efd-ac0d-4aac-801e-1fdd761450b2",
   "metadata": {},
   "source": [
    "### Inspecting the result\n",
    "The `Prediction` object generated by `deployment.infer` is equal in structure to the predictions sent by the platform. So let's have a closer look at it. We can do so in two ways: \n",
    "\n",
    "1. Visualise it using the `show_image_with_annotation_scene` utility function\n",
    "2. Inspecting its properties via the `prediction.overview` property\n",
    "\n",
    "Let's show it on the image first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde3d0c-aa3a-4635-b76c-df5c6f033de2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from geti_sdk.utils import show_image_with_annotation_scene\n",
    "\n",
    "show_image_with_annotation_scene(numpy_image, prediction, show_in_notebook=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6ca304-083a-4e93-9008-215b86c6b9d4",
   "metadata": {},
   "source": [
    "## Switching to local deployment\n",
    "Of course, we can still use the deployment to load the models locally on the client. That can be done simply by calling `deployment.load_inference_models` again, this time specifying a different device (for example `CPU` or `GPU`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3c911-af19-418d-8220-c85e4d907453",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbdf5ae-b282-411b-afc3-c6a390cccb9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "prediction = deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running local inference on image took {t_elapsed*1000:.2f} milliseconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cef1900-941d-4669-9880-d5952ee3fbcb",
   "metadata": {},
   "source": [
    "Notice that the code to run inference is exactly the same, whether it uses OVMS or loads the models directly to the CPU. \n",
    "\n",
    "## Benchmarking inference times\n",
    "\n",
    "You might have noticed that there is a difference in execution time due to the overhead introduced by OVMS. Let's do some benchmarking to further investigate the difference.\n",
    "\n",
    "First, we measure the execution time for running inference on CPU locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fabe19-5a55-451d-a54f-e250c449e9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "\n",
    "# CPU inference\n",
    "prediction = deployment.infer(numpy_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97370c-6dbb-4f13-b193-9817eba02371",
   "metadata": {},
   "source": [
    "Now switch to OVMS and run the benchmark again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62339ec9-c1cf-4a85-86b4-809f35580237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deployment.load_inference_models(device=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da1af8-672d-4ccb-ae34-4ed3f44b9be5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "\n",
    "# OVMS inference\n",
    "prediction = deployment.infer(numpy_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9c08ff-d0fb-49eb-a795-3a3dbae7a081",
   "metadata": {},
   "source": [
    "For the single task `COCO animal detection demo` project, OVMS inference introduces some overhead (the exact amount is depending on the hardware configuration of your system). Note that this does not include any network traffic yet, because OVMS is running on your local system as well: Running OVMS on a remote server will introduce additional overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ad662-90e8-4f27-b520-ec0779a322fc",
   "metadata": {},
   "source": [
    "# OVMS inference for task-chain projects\n",
    "For projects involving a task-chain, the same process can be used. In this section of the notebook, we'll create a deployment for the project created in notebook [004](004_create_pipeline_project_from_dataset.ipynb), `COCO multitask animal demo`, and do benchmarking on it. If you don't have the project on your server yet, run notebook 004 to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9171f158-1354-4214-a220-8c86e0487150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MULTITASK_PROJECT_NAME = \"COCO multitask animal demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0656fabd-cd02-4e60-9564-ba241024cd1f",
   "metadata": {},
   "source": [
    "Make sure the project is trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6819f77-de78-4c2b-bcda-57b096dd6278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mt_project = ensure_trained_example_project(\n",
    "    geti=geti, project_name=MULTITASK_PROJECT_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691904af-5603-4486-86d2-386b86abd298",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "safe_mt_project_name = sanitize_filepath(MULTITASK_PROJECT_NAME).replace(\" \", \"_\")\n",
    "\n",
    "# Target folder in which to save the deployment and OVMS configuration\n",
    "mt_output_folder = os.path.join(\"deployments\", safe_mt_project_name)\n",
    "\n",
    "# Create the deployment and OVMS configuration, and save it to the `output_folder` on disk\n",
    "multitask_deployment = geti.deploy_project(\n",
    "    project_name=MULTITASK_PROJECT_NAME,\n",
    "    prepare_ovms_config=True,\n",
    "    output_folder=mt_output_folder,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d2c2b5-ecb9-4a47-bafc-c6fe136bfb1d",
   "metadata": {},
   "source": [
    "The `COCO multitask animal demo` project contains a detection task followed by a classification task. Now, launching the OVMS docker container for the project will serve two models instead of one: One for the first task, and one for the second.\n",
    "\n",
    "The cell below will launch the container for the project, it will be listening on port 9001 since port 9000 is already occupied by the model server we created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca9c4d-a904-4d9a-bf42-5c84d83cf0a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mt_ovms_config_path = os.path.join(os.getcwd(), mt_output_folder, \"ovms_models\")\n",
    "\n",
    "! docker run -d --rm -v {mt_ovms_config_path}:/models -p 9001:9001 openvino/model_server:latest --port 9001 --config_path /models/ovms_model_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ef59a6-baf8-411f-a01b-e0620eee7348",
   "metadata": {},
   "source": [
    "## Running inference and inspecting results\n",
    "Let's check if OVMS inference for our task-chain project works. First connect to the OpenVINO model server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645f97a-6088-45b3-87f0-f17666d3623e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multitask_deployment.load_inference_models(device=\"http://localhost:9001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cbed4-c012-4b0e-afaa-a1d32ee61465",
   "metadata": {},
   "source": [
    "Then run inference on the familiar example image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2accc67-7f40-4b63-8000-590df1acb9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "prediction = multitask_deployment.infer(numpy_rgb)\n",
    "t_elapsed = time.time() - t_start\n",
    "\n",
    "print(f\"Running OVMS inference on image took {t_elapsed*1000:.2f} milliseconds\")\n",
    "\n",
    "show_image_with_annotation_scene(numpy_rgb, prediction, show_in_notebook=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a17fc3-bcb9-48d1-811f-c02969bf60a2",
   "metadata": {},
   "source": [
    "## Benchmarking inference times for the task-chain project\n",
    "Lets do the benchmarking again to get a feeling for the difference between OVMS inference and local inference. We'll start with OVMS inference this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f55ff1-f2ca-4b2d-9dbe-d4e09d6c2b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "\n",
    "# OVMS inference\n",
    "prediction = multitask_deployment.infer(numpy_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05880bf6-43d3-4ecc-afea-39ef5e0597ae",
   "metadata": {},
   "source": [
    "Now switch the deployment to load the models directly on the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26681cc5-fc42-4412-8ea5-af5d57201c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "multitask_deployment.load_inference_models(device=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f85968-0e1c-4255-9440-e574a76eaa95",
   "metadata": {},
   "source": [
    "And lets `timeit` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e99d51-aed0-44a4-b1cc-a3939370eab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "\n",
    "# CPU inference\n",
    "prediction = multitask_deployment.infer(numpy_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b5292-e5ce-4f0f-bff0-ca12c0ef9d82",
   "metadata": {
    "tags": []
   },
   "source": [
    "Also in this case you'll find that OVMS inference introduces some overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437cf26-ab69-4d9c-a31b-8f2fe9b2697a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning up\n",
    "To clean up, we'll use the `docker stop` command to stop the ovms containers that were created in this notebook. Otherwise they'll keep on running in the background.\n",
    "\n",
    "First, we get the IDs of the running OVMS containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5cca6b-fedf-4fe6-bcb2-e3afeadefbbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "container_ids = ! docker ps -q --filter ancestor=openvino/model_server\n",
    "\n",
    "print(f\"Found {len(container_ids)} running OVMS containers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0fb758-355e-4ab9-91a2-9795f20bfebd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Then, stop the containers. Stopping the container will automatically remove them (this is because of the `--rm` flag in the `docker run` command that we used to launch the containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fc7bf9-1b9d-477a-b9e0-45a4a554c70f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stop each container\n",
    "for ovms_container_id in container_ids:\n",
    "    result = ! docker stop {ovms_container_id}\n",
    "\n",
    "    if result[0] == ovms_container_id:\n",
    "        print(f\"OVMS container '{ovms_container_id}' stopped and removed successfully.\")\n",
    "    else:\n",
    "        print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c99f9-dd4f-4402-95ef-2b8e316c2f20",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "That's it! This notebook should provide a handle on how to deploy and serve models created with the Intel® Geti™ platform. \n",
    "\n",
    "The OVMS configuration files created in this notebook can be used independently: They just need to be provided to the OVMS docker container upon startup. This is useful when you aim to deploy a remote OVMS instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba2f8c1-6414-4d9f-8270-f6995425cb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c50e16b1-86a2-42f9-84be-845f06bcbb60",
   "metadata": {},
   "source": [
    "# Geti model benchmarking\n",
    "\n",
    "This notebook shows how to measure and compare the inference rates on your local hardware for the various algorithms available in a Geti project. The Geti SDK provides a `Benchmarker` class, that allows to quickly set up a series of benchmarking experiments for a specific project. This allows making a comparison of the framerates that can be achieved using deployed models of different algorithms. As such, it can help to select a suitable architecture for model deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb21e7-b203-467d-b136-3a8f6a5879b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476b474-22a2-4c78-9ccb-3b9166c36264",
   "metadata": {},
   "source": [
    "## Selecting a project\n",
    "The `Benchmarker` can run experiments for a single project. It can be either a single task or a task chain project. As an example, we'll pick the `COCO animal detection demo` project that we created in [notebook 002](002_create_project_from_dataset.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5670de-61e8-44d9-95dd-dd0a4b88e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"COCO animal detection demo\"\n",
    "project = geti.get_project(PROJECT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624baa98-d183-430e-99aa-6f1d862d5b6a",
   "metadata": {},
   "source": [
    "## Setting up the Benchmarker\n",
    "Now that we know which project to pick, we can initialize the `Benchmarker`. We need to decide on a couple of things:\n",
    "1. Which algorithms to benchmark\n",
    "2. What media to use for benchmarking\n",
    "3. The precision levels of the models we want to benchmark (i.e. FP32, FP16, INT8)\n",
    "\n",
    "### Algorithms\n",
    "For the algorithms, let's use 3 different algorithms that are available for the project that we selected. The `COCO animal detection demo` project is a single task project, with a Detection task. The `algorithms_to_benchmark` variable holds the names of the different Detection algorithms that we can choose in Geti: `SSD`, `YOLOX` and `MobileNetV2-ATSS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83641870-794d-4cbd-a108-2197d319e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms_to_benchmark = [\"MobileNetV2-ATSS\", \"SSD\", \"YOLOX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9fd76f-4d63-4a32-8eaa-dd7affd31357",
   "metadata": {},
   "source": [
    "### Media\n",
    "The experiments can run on either images or a video. There are various options to specify the input media: You can supply a list of image filepaths, a path to a video, a list of Geti `Image` objects or numpy arrays, or a Geti `Video` object. In this case, we'll simply use all images that are already in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7988bff8-fc2b-48f0-84c6-5f26b945f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients import ImageClient\n",
    "\n",
    "image_client = ImageClient(\n",
    "    session=geti.session, workspace_id=geti.workspace_id, project=project\n",
    ")\n",
    "images = image_client.get_all_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c22b75-e229-40e2-b42c-f541e8694d70",
   "metadata": {},
   "source": [
    "### Precision levels\n",
    "Geti allows deploying models with different precision levels. Typically, deploying a model with `INT8` precision results in a considerable increase in throughput compared to running an `FP32` or even `FP16` model. Using the `Benchmarker`, we can measure the inference framerate for each of these precision levels and quantify the difference. We simply have to pass a `precision_levels` variable that contains the model precisions we want to deploy and measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c3a26b-126e-454e-befe-38f3d477c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_levels = [\"FP32\", \"FP16\", \"INT8\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7dcc8-9ec7-4212-82c1-13a5d93960e0",
   "metadata": {},
   "source": [
    "### Benchmarker initialization\n",
    "With the project, algorithms, media and precision levels sorted out, we can initialize the Benchmarker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506f981-cbb9-41c4-8751-cba07592dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.benchmarking import Benchmarker\n",
    "\n",
    "benchmarker = Benchmarker(\n",
    "    geti=geti,\n",
    "    project=project,\n",
    "    algorithms=algorithms_to_benchmark,\n",
    "    precision_levels=precision_levels,\n",
    "    benchmark_images=images,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd6ede-f3f1-4705-8aa0-4eb3ff2b16ed",
   "metadata": {},
   "source": [
    "## Preparing the Intel® Geti™ project to run the benchmark\n",
    "Now that the `Benchmarker` is initialized, we need to make sure that all the algorithms that we'd like to benchmark have a model trained in the project. To do so, the Benchmarker provides a `prepare_benchmark` method that we can call. If we call it, the method will make sure of three things:\n",
    "1. It will check if every algorithm we want to benchmark has a trained model in the project. If not, it will start model training and will wait for it to complete\n",
    "2. It will check if for every trained model that we want to benchmark an optimized model is available in the specified precision levels. If not, it will trigger model optimiziation in the required precision levels and wait for it to complete\n",
    "3. It will create and download deployments for all the specified algorithms and precision levels.\n",
    "\n",
    "When calling the `prepare_benchmark` method, we just have to pass a path to a directory on the local disk. The method will save the deployments that it creates to this folder.\n",
    "\n",
    "> NOTE: Preparing the benchmark may take some time, especially if not all algorithms have a model trained. In that case we have to wait for model training to complete. Please run the cell below and wait for all jobs to complete. Progress will be reported as the training and optimization advances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463e42c-e9ea-42a0-9b2c-19c9ccd23cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "benchmark_folder = os.path.join(\"benchmarks\", PROJECT_NAME)\n",
    "benchmarker.prepare_benchmark(working_directory=benchmark_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc52522-1cde-4c7f-86a1-366b31dc7a8a",
   "metadata": {},
   "source": [
    "## Running the benchmark\n",
    "At this point all models are trained, optimized and deployed! This means that the benchmark is ready to go. You can run the benchmark by calling the `run_throughput_benchmark` method that the `Benchmarker` provides. It accepts the following arguments:\n",
    "\n",
    "- `working_directory`: The folder in which the deployments are stored that should be benchmarked. Benchmarking results will also be saved to this directory.\n",
    "- `results_filename`: The name of the file in which the benchmarking results will be saved.\n",
    "- `target_device`: The hardware that the inference models should run on. This defaults to `\"CPU\"`, but any device that is supported by OpenVINO can be used. More details can be found [here](https://docs.openvino.ai/2023.2/openvino_docs_OV_UG_supported_plugins_Supported_Devices.html).\n",
    "- `frames`: The number of video frames or images to use in the benchmark. These will be selected from the media that we provided in the benchmarking initalization early on in this notebook. Note that all frames/images will be loaded in memory for the benchmarking, so don't make this number too large or you may encounter out of memory issues.\n",
    "- `repeats`: The number of times the benchmark needs to run on all frames. Increasing this number will give a more accurate estimate of the framerate, but increases the time required for the experiments. The total number of frames that are inferred for each model is `frames * repeats`.\n",
    "\n",
    "Run the cell below to execute the benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15680806-5a03-439a-809b-35e791e509a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = benchmarker.run_throughput_benchmark(\n",
    "    working_directory=benchmark_folder,\n",
    "    results_filename=\"results\",\n",
    "    target_device=\"CPU\",\n",
    "    frames=100,\n",
    "    repeats=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce06ea12-947c-43df-8844-b867bdf07cf4",
   "metadata": {},
   "source": [
    "## Inspecting the results\n",
    "The benchmark results are stored in a `results.csv` file in the working directory that we specified earlier. In addition, the `run_throughput_benchmark` method returns the results as a list of dictionaries. This is captured in the `results` variable in the cell above. Using pandas we can easily visualize the results in the notebook.\n",
    "\n",
    "Executing the cell below should show a table containing all results from the benchmark experiments. Each row represents one of the deployments for which the benchmark ran. \n",
    "\n",
    "The most important columns are ones labelled `model 1 score`, `success` and `fps`. `model 1 score` contains the model accuracy (or F-measure in case of a detection project) for the model used in the deployment. The `success` column indicates if the deployment was able to successfully run inference on all frames. It is either `1` or `0`, with `1` indicating success. Finally, the `fps` column shows the measured average frames per second for the deployment. \n",
    "\n",
    "In addition, the table contains some details about the system, indicating the operating system, some info regarding the target device and the python, geti-sdk and openvino versions. This is useful when comparing benchmark results across different hardware setups.\n",
    "\n",
    "## Conclusion\n",
    "Ideally, the table below should help to select which model to pick for deployment in production use. The optimal model has a sufficiently high `model 1 score`, while still reaching the desired `fps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3e62e5-0b86-4c7d-a16c-49c2ebe1d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f1f09-2722-41f5-b338-e83e1cf6c11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

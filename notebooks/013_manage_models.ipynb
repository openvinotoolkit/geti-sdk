{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GETi Model Management\n",
    "\n",
    "This notebook shows how to work with models, algorithms, model groups and optimized models in a GETi project. The Geti SDK provides a `ModelClient` class, that allows to fetch a list of models and model details for a specific project, set an active model or even start an optimization job for a model. This provides a Python interface for model management in a GETi project and helps creating an optimal pipeline for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual we will connect to the platform first, using the server details from the .env file\n",
    "\n",
    "from geti_sdk import Geti\n",
    "from geti_sdk.rest_clients.project_client.project_client import ProjectClient\n",
    "from geti_sdk.utils import get_server_details_from_env\n",
    "\n",
    "geti_server_configuration = get_server_details_from_env()\n",
    "\n",
    "geti = Geti(server_config=geti_server_configuration)\n",
    "\n",
    "# We will also create a ProjectClient for the server\n",
    "project_client = ProjectClient(session=geti.session, workspace_id=geti.workspace_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of the project to download. We will use the `COCO multitask animal demo` project created in notebook [004](004_create_pipeline_project_from_dataset.ipynb).\\\n",
    "We instantiate a Project Client to retrieve the project and examine the tasks that it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.demos import ensure_trained_example_project\n",
    "\n",
    "PROJECT_NAME = \"COCO multitask animal demo\"\n",
    "ensure_trained_example_project(geti=geti, project_name=PROJECT_NAME)\n",
    "\n",
    "\n",
    "project = project_client.get_project_by_name(\"COCO multitask animal demo\")\n",
    "trainable_tasks = project.get_trainable_tasks()\n",
    "for task in trainable_tasks:\n",
    "    print(task.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the models behind the pipeline in our project. We will need a Model Client to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.model_client import ModelClient\n",
    "\n",
    "model_client = ModelClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_models = model_client.get_all_active_models()\n",
    "print(\"Active models:\")\n",
    "for i, model in enumerate(active_models):\n",
    "    print(f\"Task {i + 1}: \", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose from supported algorithms for the project.\n",
    "In GETi terminology, each model implements an algorithm. An **algorithm** serves as a blueprint for a model, defining its architecture.\\\n",
    "The **model** consists of weights and can be used for making predictions. Every model was trained with specific hyperparameters on a specific dataset and also possibly was optimized.\\\n",
    "By using a Training Client, we can explore available algorithms for the project and select the one that best suits our purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from geti_sdk.data_models.containers.algorithm_list import AlgorithmList\n",
    "from geti_sdk.rest_clients.training_client import TrainingClient\n",
    "\n",
    "training_client = TrainingClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")\n",
    "algorithms: List[AlgorithmList] = []\n",
    "for task in trainable_tasks:\n",
    "    algo_list_for_task = training_client.get_algorithms_for_task(task)\n",
    "    algorithms.append(algo_list_for_task)\n",
    "    default_algo_name = algo_list_for_task.get_default_for_task_type(\n",
    "        task.task_type\n",
    "    ).name\n",
    "    print(f\"Default algorithm for {task.title} is {default_algo_name}\\n\")\n",
    "    print(\"Other available algorithms are\")\n",
    "    print(algo_list_for_task[:4].summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model for an Algorithm\n",
    "If the project was trained with active learning system and was not modified, the default algorithms match the models in the project.\n",
    "However, we can train a new model for a heavier algorithm if we want more precision, or a lighter one if we need to increase throughput.\n",
    "\n",
    "In the next few cells we will train a new model for the first task (detection) in the pipeline.\n",
    "A user can also manipulate training hyperparameters by passing a *Taks Configuration* object. We will use this feature to change the default batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a task and algorithm we will use for training\n",
    "task_number = 0\n",
    "task_to_train = trainable_tasks[task_number]\n",
    "algo_to_train = algorithms[task_number][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.configuration_client import ConfigurationClient\n",
    "\n",
    "# Get the default task configuration from the server\n",
    "configuration_client = ConfigurationClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")\n",
    "algorithm_hyperparameters = configuration_client.get_for_task_and_algorithm(\n",
    "    task_to_train, algo_to_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update a hyperparameter and make sure the change is reflected in the configuration object\n",
    "algorithm_hyperparameters.set_parameter_value(parameter_name=\"batch_size\", value=32)\n",
    "print(algorithm_hyperparameters.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train a new model for the choosen algorithm and hyperparameters. We will use the Training Client to start a job.\n",
    "\n",
    "It may take about 10 minutes for the job to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training {task_to_train.title} with {algo_to_train.name} algorithm\\n\")\n",
    "job = training_client.train_task(\n",
    "    algorithm=algo_to_train,\n",
    "    task=task_to_train,\n",
    "    hyper_parameters=algorithm_hyperparameters,\n",
    ")\n",
    "training_client.monitor_job(job);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the active models for the project again to see the newly trained model automaticaly set by the server as the active model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_models = model_client.get_all_active_models()\n",
    "print(\"Active models:\")\n",
    "for i, model in enumerate(active_models):\n",
    "    print(f\"Task {i + 1}: \", model.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a Model\n",
    "If we want to increase the model's throughput or try out different precision levels, we can optimize the model using [OpenVINO Post Training Optimization feature](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/quantizing-models-post-training.html).\\\n",
    "We will use the Model Client to start an optimization job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = model_client.optimize_model(model=active_models[0], optimization_type=\"pot\")\n",
    "_ = model_client.monitor_job(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with model groups\n",
    "Model Group is another entity in GETi that embraces all the trained models for a specific algorithm and task, differentiating them by version name. Working with model groups allows to access different model versions and switch between them if needed, seting them active at the Platform server.\n",
    "\n",
    "The Model Client allows exploring model groups that are present in a project. If a model for some algorithm was trained for any task, the corresponding model will exist in the project.\\\n",
    "Model Group actually operates models in a shortened form to increase SDK performance and lower server load, thus to retrieve the full model we need to make an additional call to Model Client.\n",
    "\n",
    "> Note: you can reuse a model object from any of the previous steps, the model is retrieved using the Model Group further only for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_groups = model_client.get_all_model_groups()\n",
    "print(\"Model groups:\")\n",
    "for i, model_group in enumerate(model_groups):\n",
    "    print(f\"{i + 1}. \", model_group.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can restore a representation object for a specific model from the model group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_summary = model_groups[1].models[0]\n",
    "model = model_client.update_model_detail(model_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember we performed an optimization job for the model? Now we can also retrieve the optimized model from the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model = model.get_optimized_model(optimization_type=\"pot\")\n",
    "optimized_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the server to return a model in a lowered precision format, which can be used for deployment on some edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_optimized_model(precision=\"FP16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to change the active model on the server side, we can choose a model (or a model summary) from a desired model group and set it as active using the Model Client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client.set_active_model(model_groups[0].models[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "We can also choose what model to use in a local deployment.\\\n",
    "We will create a Deployment Client and call the `deploy_project` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geti_sdk.rest_clients.deployment_client import DeploymentClient\n",
    "\n",
    "deployment_client = DeploymentClient(\n",
    "    workspace_id=geti.workspace_id, project=project, session=geti.session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `deploy_project` method can accept a list of models for every task. In our case, we are ok with deploying the active model for the second task, so we will only pass a model for the first task.\n",
    "For instance, we can use the optimized model from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_client.deploy_project(models=[optimized_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the deployment is ready for inference, you can refer to [008](008_deploy_project.ipynb) for more details on how to perform inference on the deployed project.\n",
    "\n",
    "## Summary\n",
    "In this notebook we made an impressive journey through the GETi model management capabilities. We explored the models, algorithms, model groups and optimized models in a GETi project. We trained a new model from a chosen algorithm for the detection task in the pipeline, optimized and deployed it. We also worked with model groups and discovered a way to navigate around different model versions in a project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
